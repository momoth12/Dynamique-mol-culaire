{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modal SNA MAP473D, Ecole Polytechnique, 2022-23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Soumission du TP pour √©valuation:** \n",
    "- Remplir ce notebook et d√©poser sur le moodle le fichier notebook \".ipynb\" ainsi qu'une sauvegarde (export) au format \".html\".\n",
    "- Les r√©ponses aux questions th√©oriques peuvent √™tre saisies (en latex) dans le notebook; un (seul) fichier au format .pdf contenant le scan de vos r√©ponses manuscrites est aussi accept√©.\n",
    "- Les d√©p√¥ts sur le moodle doivent √™tre faits avant le vendredi 24 mars, 18h. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Noms du bin√¥me:** _A REMPLIR_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3 - Echantillonnage d‚Äôimportance : changements de probabilit√©s gaussiens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\R{\\mathbb{R}}$\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "$\\def\\mc{\\mathcal}$\n",
    "$\\def\\E{\\mathbb{E}}$\n",
    "\n",
    "## Partie 1. Rappel de cours : changements de probabilit√©s, √©chantillonnage pr√©f√©rentiel et grandes d√©viations\n",
    "Soit $X$ une v.a. √† valeur dans $\\mathbb{R}^d$, d√©finie sur un espace de probabilit√© $(\\Omega, \\mathcal{F}, \\P)$ et de loi $\\nu$ sous $\\P$.  On notera $\\mathbb{E}$ l'esp√©rance associ√©e √† la probabilit√© $\\P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\R{\\mathbb{R}}$\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "$\\def\\Q{\\mathbb{Q}}$\n",
    "$\\def\\mc{\\mathcal}$\n",
    "$\\def\\E{\\mathbb{E}}$\n",
    "$\\def\\rmd{\\mathrm{d}}$\n",
    "\n",
    "\n",
    "### 1.1 Principe g√©n√©ral du changement de probabilit√©\n",
    "\n",
    "Supposons que l'objectif soit de calculer \n",
    "$$\n",
    "\\mathbb{E}\\left[ h(X) \\right] = \\int h(x) \\, \\nu(\\mathsf{d} x)\n",
    "$$\n",
    "pour une fonction $h$ mesurable telle que cette esp√©rance est bien d√©finie. Nous supposons √™tre dans une situation o√π cette int√©grale n'est pas explicite. \n",
    "\n",
    "Une approximation num√©rique bas√©e sur la m√©thode de Monte  Carlo standard consiste √† d√©finir l'estimateur\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{k=1}^N h(X_k) \\qquad \\qquad X_k \\stackrel{i.i.d.}{\\sim} \\nu(\\mathsf{d} x).\n",
    "$$\n",
    "La variance de cet estimateur est \n",
    "$$\n",
    "\\frac{1}{N} \\mathrm{Var}(h(X)) = \\frac{1}{N} \\left( \\int h^2(x) \\nu(\\mathsf{d} x) - \\left(  \\int h(x) \\nu(\\mathsf{d} x) \\right)^2 \\right).\n",
    "$$\n",
    "Dans la suite, nous serons dans des situations o√π cet estimateur Monte Carlo standard a un pauvre comportement, et envisagerons un changement de loi pour d√©finir une m√©thode Monte Carlo plus efficace : \n",
    "\n",
    "- nous chercherons une loi $\\tilde \\nu$, telle que $\\nu$ poss√®de une densit√© $L$ par rapport √† la loi $\\tilde \\nu$; \n",
    "$$\n",
    "\\nu(\\mathsf{d} x) = L(x) \\, \\tilde \\nu(\\mathsf{d} x) \\qquad \\text{avec} \\ \\ \\int L(x) \\tilde \\nu(\\mathsf{d} x) = 1.\n",
    "$$\n",
    "- nous exploiterons la relation \n",
    "$$\\tag{1}\n",
    "\\mathbb{E}\\left[h(X) \\right] = \\int h(x) \\nu(\\mathsf{d} x) = \\int h(x) \\  L(x) \\ \\tilde \\nu(\\mathsf{d} x) = \\mathbb{E}_{\\tilde \\nu} \\left[h(X)  \\  L(X)\\right]\n",
    "$$ \n",
    "o√π sous $\\P_{\\tilde \\nu}$, $X \\sim \\tilde \\nu$. \n",
    "\n",
    "- nous en d√©duirons un nouvel estimateur Monte Carlo de la quantit√© inconnue $\\mathbb{E}[h(X)]$ : \n",
    "$$\\tag{2}\n",
    "\\frac{1}{N} \\sum_{k=1}^N h(X_k) \\, L(X_k) \\qquad X_k \\stackrel{i.i.d.}{\\sim} \\tilde \\nu. \n",
    "$$\n",
    "La variance de cet estimateur est \n",
    "$$\n",
    "\\frac{1}{N}\\mathrm{Var}_{\\tilde \\nu}(h(X) \\, L(X)) = \\frac{1}{N} \\left( \\int h^2(x) L^2(x) \\,  \\tilde \\nu(\\mathsf{d} x) - \\left(  \\int h(x) \\nu(\\mathsf{d} x) \\right)^2 \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\P{\\mathbb{P}}$\n",
    "$\\def\\Q{\\mathbb{Q}}$\n",
    "$\\def\\Vth{\\mathrm{Var}_\\theta}$\n",
    "\n",
    "### 1.2 Changements de probabilit√© inspir√©s par les grandes d√©viations (cas $d=1$)<a id='sec:12'></a>\n",
    "Dans ce qui suit, nous allons consid√©rer un changement de loi de la forme\n",
    "$$\\tag{3}\n",
    "\\nu_\\theta(\\mathsf{d} x) \\propto \\exp(\\theta x ) \\nu(\\mathsf{d} x) = \\frac{\\exp(\\theta x )}{\\int \\exp(\\theta y ) \\nu(\\mathsf{d} y)} \\,  \\nu(\\mathsf{d} x)=  \\frac{\\exp(\\theta x )}{\\mathbb{E}[\\exp(\\theta X)]}\\,  \\nu(\\mathsf{d} x)\n",
    "$$\n",
    "pour $\\theta \\in \\mathcal{D} \\subseteq \\mathbb{R}$, o√π $\\mathcal{D}$ est d√©fini comme l'ensemble de valeurs pour lesquelles la constante de normalisation est d√©finie (nous consid√©rons des lois $\\nu$ pour lesquelles cet ensemble est non vide)\n",
    "$$ \n",
    "    \\mathcal{D} := \\{ \\theta \\in \\mathbb{R}~:  \\mathbb{E}\\!\\left[e^{\\theta X} \\right]<\\infty \\}.\n",
    "$$  \n",
    "On peut montrer que $\\mathcal{D}$ est convexe. \n",
    "\n",
    "Pour tout $\\theta \\in \\mathcal{D}$, la densit√© de $\\nu$ par rapport √† $\\nu_\\theta$ est\n",
    "$$\n",
    "L_\\theta(x) := \\exp(- \\theta x) \\exp\\left( \\ln \\mathbb{E}\\left[ \\exp(\\theta X) \\right] \\right) = \\exp(-\\theta x + \\psi(\\theta))\n",
    "$$\n",
    "en ayant d√©fini la _fonction g√©n√©ratrice des cumulants_ de $\\nu$ par\n",
    "$$\n",
    " \\psi(\\theta) := \\log \\mathbb{E}\\!\\left[ e^{\\theta X} \\right] = \\log \\int \\exp(\\theta x) \\, \\nu(\\mathsf{d} x).\n",
    "$$\n",
    "Nous nous poserons la question du choix adequat de la valeur $\\theta$ parmi cet ensemble $\\mathcal{D}$.\n",
    "\n",
    "\n",
    "\n",
    "Nous introduisons une notation pour traduire le fait que nous consid√©rons des v.a. sous la loi $\\nu_\\theta$ : nous √©crirons pour tout fonction mesurable born√©e $h$\n",
    "$$\n",
    "\\mathbb{E}_\\theta\\left[h(X)\\right] : = \\int h(x) \\, \\nu_\\theta(\\mathsf{d} x).\n",
    "$$\n",
    "Avec ces notations, la relation  (1)  s'√©crit\n",
    "$$\n",
    "\\mathbb{E}\\left[h(X)\\right] = \\mathbb{E}_\\theta \\left[ h(X) \\, L_\\theta(X) \\right] = \\int h(x) \\exp(-\\theta x + \\psi(\\theta))  \\ \\nu_\\theta(\\mathsf{d} x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On peut montrer que pour tout $\\theta$ dans l'int√©rieur de $\\mathcal{D}$, l'esp√©rance et la variance de le nouvelle loi $\\nu_\\theta$ sont li√©es aux d√©riv√©es successives de $\\psi$ (voir par exemple le Corollaire 7.2  dans __[Information and Exponential Families](https://onlinelibrary.wiley.com/doi/book/10.1002/9781118857281)__) \n",
    "<a id='eq_esp_var2'></a>\n",
    "$$\\tag{4}\n",
    "    \\mathbb{E}_\\theta[X]=\\psi'(\\theta),\\qquad \\qquad \n",
    "    \\mathrm{Var}_\\theta(X)=\\psi''(\\theta);\n",
    "$$\n",
    "et en particulier, pour $\\theta = 0$,\n",
    "$$ \n",
    "    \\mathbb{E}[X]=\\psi'(0),\\qquad \\qquad \n",
    "    \\mathrm{Var}(X)=\\psi''(0).\n",
    "$$\n",
    "\n",
    "Le succ√®s de cette technique de changement de loi d√©pend de la capacit√© √† identifier la loi $\\nu_\\theta$ et √† faire des tirages i.i.d. sous cette loi.  Par exemple, nous montrerons en section 3.1 que si $\\nu$ est la loi $\\mathcal{N}(0,1)$, alors $\\nu_\\theta$ est la loi  $\\mathcal{N}(\\theta,1)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "### 1.3 Application √† l'approximation Monte Carlo de la fonction de survie\n",
    "Supposons que l'objectif soit d'approcher la quantit√© $$\\P[X>a] = \\mathbb{E}\\left[1_{X > a} \\right]$$ par une m√©thode de Monte Carlo. \n",
    "\n",
    "\n",
    "L'approximation $(2)$ devient\n",
    "<a id='eq:MC:Ptheta'></a>\n",
    "$$ \\tag{5}\n",
    "    \\P(X>a) \\simeq \\frac{1}{n} \\sum_{k=1}^n \\exp\\left( \\psi(\\theta) - \\theta \\, \\tilde X_k \\right) \\, 1_{\\tilde X_k>a}, \\qquad \\qquad \\tilde X_k \\stackrel{i.i.d.}{\\sim} \\nu_\\theta.\n",
    "    $$ \n",
    " \n",
    "\n",
    "La difficult√© pour l'utilisateur est de choisir la valeur $\\theta \\in \\mathcal{D}$ la plus \"efficace\".  La relation [(4)](#eq_esp_var2) sugg√®re de choisir $\\theta$ √©gal √† la solution de $\\psi'(\\theta) = a$ (valeur que nous noterons $\\theta_a$), puisqu'avec ce choix, la nouvelle loi $\\nu_\\theta$ est centr√©e en $a$. Ce n'est pas n√©cessairement le choix optimal - selon un crit√®re d'optimalit√© bas√© sur la r√©duction de variance (voir section 3.1) - mais cela donne un √©chantillonneur Monte Carlo plus efficace que l'√©chantillonneur Monte Carlo standard puisqu'il a plus de chances de produire des points $\\tilde X_k$ qui v√©rifient la condition $\"x > a\"$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "## Partie 2. Echantillonnage d'importance pour des v.a. de Bernoulli\n",
    "Les v.a. sont d√©finies sur $(\\Omega, \\mathcal{A}, \\P)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\P{\\mathbb{P}}$\n",
    "#### <span style=\"color:blue\">Question 1 (th√©orique). </span> \n",
    "Sous $\\mathbb{P}_{p_\\cdot}$, $X_1, \\cdots, X_n$ sont des v.a. ind√©pendantes de loi de Bernoulli $B(p_1), \\ldots, B(p_n)$.\n",
    "\n",
    "Sous $\\mathbb{P}_{q_\\cdot}$, $X_1, \\ldots, X_n$ des v.a. ind√©pendantes de lois de Bernoulli $B(q_1), \\ldots, B(q_n)$.\n",
    "\n",
    "Montrer que pour toute fonction $h:\\{0,1\\}^n \\to\\mathbb{R}$ mesurable born√©e, on a\n",
    "\\begin{eqnarray*} \n",
    "  \\mathbb{E}_{p_\\cdot} \\left[ h(X_1, \\ldots, X_n) \\right] &=& \\mathbb{E}_{q_\\cdot} \\left[ h(X_1, \\ldots, X_n) \\, \\prod_{i=1}^n \\left(1_{X_i=0}\\frac{1-p_i}{1-q_i}+ 1_{X_i=1}\\frac{p_i}{q_i}\\right) \\right]\n",
    "  \\\\\n",
    "                                        &=& \\left( \\prod_{i=1}^n\\frac{1-p_i}{1-q_i} \\right) \\  \\mathbb{E}_{q_\\cdot}\\left[ h(X_1, \\ldots,X_n) \\ \\prod_{i=1}^n\\left(\\frac{p_i(1-q_i)}{q_i(1-p_i)}\\right)^{X_i} \\right]\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _R√©ponse:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "***\n",
    "#### <span style=\"color:blue\">Question 2 (th√©orique). \n",
    "    \n",
    "Soit $n\\ge 1$, $p,q\\in ]0,1[$; sous $\\mathbb{P}_p$,  $S \\sim \\operatorname{Binom}(n,p)$ et sous $\\mathbb{P}_q$, $S \\sim \\operatorname{Binom}(n,q)$.\n",
    "\n",
    "\n",
    "D√©duire de la question pr√©c√©dente que pour toute fonction $h:\\{0, \\ldots, n\\} \\to \\mathbb{R}$, on a\n",
    "$$\n",
    "    \\mathbb{E}_p \\left[ h(S) \\right] \n",
    "    = \\mathbb{E}_q \\left[ h(S) \\ \\left(\\frac{p}{q}\\right)^{S}\\left(\\frac{1-p}{1-q}\\right)^{n-S} \\right] \n",
    "    = \\left(\\frac{1-p}{1-q}\\right)^n \\mathbb{E}_q\\left[h(S) \\ \\left(\\frac{p(1-q)}{q(1-p)}\\right)^{S} \\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _R√©ponse:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\P{\\mathbb{P}}$\n",
    "***\n",
    "#### <span style=\"color:blue\">Question 3. </span> \n",
    "\n",
    "Soit $S \\sim \\operatorname{Binom}(n,p)$ et $x \\in ]0,p[$.  Estimer $\\P(S \\le nx)$ des trois fa√ßons suivantes:\n",
    "- en utilisant la fonction de calcul exact de cette probabilit√© `scipy.stats.binom.cdf`,\n",
    "- par un Monte Carlo standard en simulant $N$ copies de $S$ de loi $\\operatorname{Binom}(n,p)$ avec la fonction `np.random.binomial` ou `scipy.stats.binom.rvs`,\n",
    "- en utilisant la question 2 et en simulant $N$ copies de $S$ de loi $\\operatorname{Binom}(n,q)$.\n",
    "\n",
    "Dans les deux derniers cas, on donnera un intervalle de confiance asymptotique de probabilit√© de couverture $0.95$, ainsi que l'erreur relative.\n",
    "\n",
    "On pourra choisir par exemple $n=300$, $p=0.25$, $x=0.1$, $q=x$ et $N=10^4$; et recommencer avec $N=10^6$. \n",
    "\n",
    "Plus g√©n√©ralement, proposer une analyse qui justifie la choix de $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "## Partie 3. Changement de probabilit√© dans un mod√®le gaussien\n",
    "Dans cette section, $\\nu$ est  la loi $\\mathcal{N}(0,1)$.  Sous $\\P$, $X \\sim \\mathcal{N}(0,1)$.\n",
    "\n",
    "\n",
    "Nous cherchons √† calculer la probabilit√© d'√©venements de la forme $\\{X>a\\}$ ou $\\{|X|>a\\}$ pour des valeurs assez √©lev√©es de $a$ de sorte que l'√©v√©nement puisse √™tre qualifi√© de rare. \n",
    "\n",
    "Nous avons la relation, pour tout $a>0$,\n",
    "$$\n",
    "    \\frac{1}{a+1/a} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{a^2}{2}} \n",
    "    \\le \\P(X>a) \\le \n",
    "    \\frac{1}{a} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{a^2}{2}},\n",
    "$$\n",
    "ce qui entraine quand $a \\to +\\infty$,\n",
    "$$ \n",
    "    \\P(X>a) \\simeq \\frac{1}{a }\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{a^2}{2}}.\n",
    "$$\n",
    "Ainsi, comme $e^{\\frac{2.15^2}{2}} \\simeq 10$, $\\P(X>a)$ et $\\P(|X|>a)$ sont de l'ordre de $10^{-\\left(\\frac{a}{2.15}\\right)^2}$: des valeurs de $a$ comprises dans l'intervalle $[5,8]$ correspondent √† notre champ d'investigation. \n",
    "\n",
    "\n",
    "Nous consid√©rons deux changements de probabilit√© pour am√©liorer le comportement de l'√©chantillonneur de Monte Carlo na√Øf donn√© par \n",
    "$$\\tag{6}\n",
    "    \\P(X>a) \\simeq \\frac{1}{n} \\sum_{k=1}^n 1_{X_k > a} \\qquad  X_k \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(0,1).\n",
    "$$\n",
    "$$ \n",
    "\\P(|X|>a) \\simeq \\frac{1}{n} \\sum_{k=1}^n 1_{|X_k| > a} \\qquad  X_k \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(0,1).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "### 3.1 Changement de probabilit√© par d√©centrage\n",
    "Ce changement de probabilit√© repose sur le changement de loi $\\nu \\to \\nu_\\theta$ d√©crit en [section 1.2](#sec:12).  \n",
    "\n",
    "La fonction g√©n√©ratrice des cumulants de $\\nu$ est d√©finie sur $\\mathcal{D} = \\mathbb{R}$ et vaut $\\psi(\\theta) =\\theta^2/2$. \n",
    "\n",
    "Identifions la loi $\\nu_\\theta$: la relation [(3)](#eq:Principe:IS) montre que pour toute fonction $h:\\mathbb{R} \\to \\mathbb{R}_+$ mesurable\n",
    "\\begin{align*}\n",
    " \\int h(x) \\ \\nu_\\theta(\\mathsf{d} x)   &= \\frac{1}{\\sqrt{2\\pi}} \\int_{\\mathbb{R}} h(x) \\exp \\left(\\theta x - \\theta^2/2 \\right) \\exp(-x^2/2) \\mathsf{d} x  \\\\\n",
    "  &= \\frac{1}{\\sqrt{2\\pi}} \\int_{\\mathbb{R}} \\, h(x) \\exp \\left(-(x-\\theta)^2/2 \\right) \\, \\mathsf{d} x.\n",
    "\\end{align*}\n",
    "On en d√©duit que $\\nu_\\theta$ est la loi $\\mathcal{N}(\\theta,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "#### <span style=\"color:blue\">Question 4. </span> \n",
    "Simuler $n \\gg 1$ gaussiennes ind√©pendantes de loi $\\mathcal{N}(0,1)$ avec lesquelles on estimera $\\P(X>a)$ d'abord par l'estimateur na√Øf [(6)](#eq:MC:naif), puis par l'estimateur [(5)](#eq:MC:Ptheta) appliqu√© avec $\\theta = \\theta_a$ o√π \n",
    "$$ \\theta_a: \\text{solution de} \\qquad \\psi'(\\theta) = a. $$  \n",
    "\n",
    "- Comparer √† la valeur exacte de $\\P(X>a)$.  \n",
    "- Pour chacun des estimateurs, utiliser le TCL pour construire un intervalle de confiance asymptotique de probabilit√© de couverture $95 \\%$. \n",
    "- Lorsque cela s'y pr√™te, regarder la qualit√© de l'estimation par exemple en calculant l'erreur relative (taille de l'intervalle de confiance divis√©e par l'estimation ponctuelle).\n",
    "- Expliquez comment vous avez choisi le nombre de termes $n$ pour construire ces estimateurs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "#### <span style=\"color:blue\">Question 5 (th√©orique). </span> \n",
    "\n",
    "La longueur de l'intervalle de confiance est une fonction croissante de la variance de l'estimateur : on veut donc choisir $\\theta$ qui minimise cette variance.  Nous allons montrer que le choix de $\\theta$ fait dans la question pr√©c√©dente pourrait √™tre am√©lior√© si le crit√®re d'optimalit√© souhait√© est de minimiser la variance du nouvel estimateur.\n",
    "\n",
    "a) Montrer que choisir $\\theta$ qui minimise la variance de l'estimateur donn√© par [(5)](#eq:MC:Ptheta), revient √† choisir $\\theta$ qui minimise\n",
    "$$\n",
    "    \\theta \\mapsto \\mathbb{E}\\left[ 1_{X > a} \\exp\\left(\\frac{\\theta^2}{2} - \\theta X \\right) \\right].\n",
    "$$\n",
    "\n",
    "b) Montrer que cette fonction est convexe et qu'elle poss√®de un unique minimum atteint en $\\theta_\\star$ solution de\n",
    "$$\n",
    "\\text{$\\theta$ tel que }  \\qquad  \\mathbb{E}\\left[ 1_{X > a} (\\theta - X) \\exp\\left(- \\theta X \\right) \\right] = 0;\n",
    "$$\n",
    "on admettra que l'on peut permuter d√©riv√©e et int√©gration.\n",
    "\n",
    "c) En d√©duire que prendre $\\theta = \\theta_a$ est meilleur que prendre $\\theta = 0$; et que l'on a $\\theta_\\star > \\theta_a$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _R√©ponse:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "\n",
    "### 3.2 Changement de probabilit√© par modification de la variance\n",
    "Le changement de probabilit√© par modification de la variance repose sur l'observation suivante : pour toute fonction $h: \\mathbb{R} \\to \\mathbb{R}$ mesurable born√©e, on a\n",
    "$$\n",
    "    \\mathbb{E}[h(X)] =  \\frac{1}{\\sqrt{2 \\pi}} \\int  h(y) e^{- \\frac{1}{2} y^2} \\,  \\mathsf{d} y =   \\frac{\\sigma}{ \\sigma \\sqrt{2 \\pi}} \\int  h(y) e^{- \\frac{1}{2} y^2  + \\frac{1}{2\\sigma^2} y^2} \\,e^{-  \\frac{1}{2\\sigma^2} y^2}  \\mathsf{d} y = \\sigma \\, \\mathbb{E}_\\sigma \\left[h(X)     e^{- \\frac{1}{2} X^2  + \\frac{1}{2\\sigma^2} X^2} \\right]\n",
    "$$\n",
    "o√π sous $\\P_\\sigma$,  $X \\sim \\mathcal{N}(0, \\sigma^2)$. \n",
    "\n",
    "On peut donc estimer $\\mathbb{E}[h(X)]$ par \n",
    "<a id='eq:MC:Psi'></a>\n",
    "$$ \n",
    "    \\mathbb{E}[h(X)] \\simeq \\frac{\\sigma}{n} \\sum_{k=1}^n \\, h(Y_k) \\, e^{-\\frac{(\\sigma^2-1)}{2 \\sigma^2} Y_k^2} , \\qquad\n",
    "   Y_k \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(0,\\sigma^2).\n",
    "$$\n",
    "L√† encore, le choix de $\\sigma$ n'est pas anodin sur la qualit√© de l'estimateur. Chercher $\\sigma$ qui minimise la variance de l'estimateur i.e.\n",
    "$$\n",
    "\\frac{1}{n} \\mathrm{Var}_\\sigma \\left( \\sigma \\, h(Y) \\, e^{-\\frac{(\\sigma^2-1)}{2 \\sigma^2} Y^2}\\right)\n",
    "$$\n",
    "est √©quivalent √† minimiser le moment d'ordre $2$  (puisque l'estimateur est sans biais) soit \n",
    "$$\n",
    "\\sigma^2 \\, \\mathbb{E}_\\sigma \\left[ h^2(Y) \\, e^{-\\frac{(\\sigma^2-1)}{\\sigma^2} Y^2}\\right] = \\sigma^2 \\, \\mathbb{E}\\left[ h^2(\\sigma X) \\, e^{-(\\sigma^2-1) X^2}\\right]. \n",
    "$$\n",
    "On choisira $\\sigma = \\sigma_\\star$ o√π $\\sigma_\\star$ est solution de\n",
    "$$\n",
    "    \\operatorname{argmin}_{\\sigma > 0} \\sigma^2 \\ \\mathbb{E} \\left[ h^2(\\sigma X)  \\ e^{-(\\sigma^2-1)X^2} \\right].\n",
    "$$\n",
    "Bien souvent, on n'a pas d'expression explicite de cette valeur optimale, et la d√©termination de $\\sigma_\\star$ est une des principales difficult√©s de l'impl√©mentation de cette m√©thode. Dans l'application num√©rique suivante, nous utiliserons une technique rudimentaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "#### <span style=\"color:blue\">Question 6. </span> \n",
    "Se donner $\\mathcal{S}$, une version discr√©tis√©e de l'intervalle $\\left[\\frac{a}{2}, 3a \\right]$.  \n",
    "\n",
    "Simuler $n_1$ gaussiennes centr√©es r√©duites.\n",
    "\n",
    "Avec ce m√™me jeu de simulations, pour chaque valeur $\\sigma \\in \\mathcal{S}$, calculer une approximation Monte Carlo de la quantit√© \n",
    "$$\n",
    "    \\sigma^2 \\ \\mathbb{E} \\left[ 1_{|\\sigma X| >a} e^{-(\\sigma^2-1)X^2} \\right].\n",
    "$$\n",
    "En d√©duire une approximation $\\hat{\\sigma}_\\star$ de $\\sigma_\\star$.\n",
    "\n",
    "*Indication : on pourra consid√©rer une version discr√©tis√©e de l'intervalle en $300$ points √©quidistants. Puis on pourra relancer la recherche sur un intervalle plus r√©duit. Commencez par prendre $n=1e4$ puis l'augmenter - jusqu'√† prendre $ùëõ_1$ de l'ordre de $1ùëí6$ et m√™me plus grand. Commentez*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\P{\\mathbb{P}}$\n",
    "#### <span style=\"color:blue\">Question 7. </span>  \n",
    "Simuler $n_2$ autres gaussiennes centr√©es r√©duites $X_1, \\dots, X_{n_2}$ ($n_2 \\gg n_1$). \n",
    "\n",
    "Avec ce nouveau jeu de simulations, \n",
    "- proposer une estimation de $\\P(|X| >a)$ via la m√©thode de Monte Carlo na√Øve; \n",
    "- puis une estimation via la formule\n",
    "$$\n",
    "    \\P(|X|>a) \\simeq \\frac{\\sigma}{n_2} \\sum_{k=1}^{n_2} 1_{|\\sigma X_k| > a} e^{-\\frac{(\\sigma^2-1)}{2} X_k^2} \\qquad X_k \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(0,1)\n",
    "$$\n",
    "tout d'abord pour $\\sigma = \\hat{\\sigma}_\\star$ puis pour $\\sigma = a$. Cette formule utilise le fait que si $X \\sim \\mathcal{N}(0,1)$ alors $\\sigma X \\sim \\mathcal{N}(0, \\sigma^2)$. \n",
    "\n",
    "Pour chacun des cas,  \n",
    "- utiliser le TCL pour construire un intervalle de confiance asymptotique de probabilit√© de couverture $95\\%$.  \n",
    "- afficher l'erreur relative, lorsqu'elle est bien d√©finie.\n",
    "\n",
    "Comparer ces strat√©gies, par exemple en regardant l'erreur relative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "## Partie 4. Algorithme adaptatif de moyenne optimale dans le cas gaussien multidimensionnel\n",
    "\n",
    "Dans cette section, $I_d$ d√©signe la matrice identit√© de taille $d \\times d$. On note  $\\|\\cdot \\|$ la norme euclidienne associ√©e. Pour un vecteur $a$, $a^\\top$ est la transpos√©e. Par convention, les vecteurs sont des vecteurs colonnes.\n",
    "\n",
    "Sous $\\P$, $X$ est un vecteur gaussien standard de $\\mathbb{R}^d$ : $\\nu$ est donc la loi $\\mathcal{N}(0,I_d)$. \n",
    "\n",
    "On cherche √† calculer $\\mathbb{E}[h(X)]$ pour une fonction $h : \\mathbb{R}^d \\to \\mathbb{R}$ mesurable telle que pour tout $\\theta\\in \\mathbb{R}^d$,\n",
    "$$\n",
    "    0<\\mathbb{E} \\left[h^2(X) \\ e^{- \\theta^\\top X} \\right]<\\infty.\n",
    "$$\n",
    "Pour ce faire, on peut impl√©menter un estimateur de Monte Carlo na√Øf ou exploiter la relation\n",
    "<a id='eq:IS:multidim'></a>\n",
    "$$ \\tag{7}\n",
    "  \\mathbb{E}[h(X)]=\\mathbb{E} \\left[h(X + \\theta) \\, e^{- \\theta^\\top  X-\\|\\theta\\|^2/2} \\right],\n",
    "$$ \n",
    "valable pour tout $\\theta \\in \\mathbb{R}^d$, et toute fonction $h: \\mathbb{R}^d \\to \\mathbb{R}_+$ mesurable. L'objectif de cette section est d'apprendre, par une proc√©dure adaptative, la valeur optimale du param√®tre $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "\n",
    "#### Algorithme de Lelong et Jourdain\n",
    "D'apr√®s l'article [Robust adaptive importance sampling for normal random vectors](https://projecteuclid.org/euclid.aoap/1255699541). On d√©duit de [(7)](#eq:IS:multidim) que pour tout $\\theta \\in \\mathbb{R}^d$,\n",
    "- (i) la v.a.\n",
    "<a id='eq:IS:JL'></a>\n",
    "$$ \\tag{8}\n",
    "    M_n(\\theta) := \\frac{1}{n} \\sum_{k=1}^n h(X_k + \\theta) \\ e^{-\\theta^\\top X_k -\\|\\theta\\|^2/2}, \n",
    "    \\qquad  X_k \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(0,1)\n",
    "$$\n",
    "est un estimateur sans biais de l'objectif $\\mathbb{E}\\left[h(X)\\right]$,\n",
    "- (ii) sa variance est √©gale √† $n^{-1} \\left( \\mathcal{V}(\\theta)- \\left(\\mathbb{E}[h(X)] \\right)^2 \\right)$, o√π\n",
    "$$\n",
    "    \\mathcal{V}(\\theta) := \\mathbb{E} \\left[h^2(X) \\,  e^{- \\theta^\\top X +\\|\\theta\\|^2/2} \\right].\n",
    "$$\n",
    "\n",
    "On souhaite donc appliquer l'estimateur [(8)](#eq:IS:JL) avec $\\theta \\leftarrow \\theta_\\star$ o√π $\\theta_\\star$ minimise $\\mathcal{V}(\\theta)$. Cependant, en g√©n√©ral, lorsque $\\mathbb{E}[h(X)]$ est inconnu, il en est de m√™me pour $\\mathcal{V}(\\theta)$ et donc $\\theta_\\star$ est d√©fini comme un minimiseur d'une fonction incalculable. Or, sous des conditions d'int√©grabilit√© sur $h$, la fonction $\\mathcal{V}$ est de classe $C^2$, strictement convexe et poss√©dant un unique minimum $\\theta_\\star$; son gradient $G$ et son Hessien $H$ sont donn√©s par\n",
    "\\begin{align*}\n",
    "  G(\\theta) & :=  \\mathbb{E} \\left[ h^2(X) \\left( \\theta -X \\right) e^{- \\theta^\\top X +\\|\\theta\\|^2/2} \\right]  \\in \\mathbb{R}^{d \\times 1}, \\\\\n",
    "  H(\\theta) & := \\mathbb{E} \\left[ h^2(X) \\left( I_d + (\\theta-X) (\\theta-X)^\\top \\right) e^{- \\theta^\\top X+\\|\\theta\\|^2/2} \\right]  \\in \\mathbb{R}^{d \\times d}.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Afin d'approcher $\\theta_\\star$, on va utiliser une version bruit√©e de la *m√©thode de Newton* : pour $n$ fix√©, $\\theta_\\star$ sera approch√© par $\\theta_n$ d√©fini comme la limite lorsque $k$ tend vers l'infini, de la suite $\\{t_k, k\\ge 0\\}$ satisfaisant √†\n",
    "<a id='eq:Newton'></a>\n",
    "$$ \\tag{9}\n",
    "    t_{k+1} = t_k - \\left(H_n(t_k)\\right)^{-1} \\, G_n(t_k)\n",
    "$$\n",
    "o√π $H_n(t), G_n(t)$ sont les approximations Monte Carlo de $H(t)$ et $G(t)$ calcul√©es √† partir des m√™mes $n$ r√©alisations ind√©pendantes $X_1, \\cdots, X_n$ de v.a. de loi $\\mathcal{N}_d(0,I_d)$:\n",
    "\\begin{align*}\n",
    "    G_n(\\theta) & :=  \\frac{1}{n}\\sum_{k=1}^n h^2(X_k) (\\theta-X_k)e^{- \\theta^\\top X_k +\\|\\theta\\|^2/2}, \\\\\n",
    "    H_n(\\theta) & :=  \\frac{1}{n}\\sum_{k=1}^n h^2(X_k) \\left(I_d + (\\theta-X_k)(\\theta-X_k)^\\top\\right) e^{-\\theta^\\top X_k+\\|\\theta\\|^2/2}.\n",
    "\\end{align*}\n",
    "En pratique, on se fixe un seuil $\\varepsilon > 0$ et on it√®re la relation [(9)](#eq:Newton) tant que $\\|G_n(t_k)\\|>\\varepsilon$. On obtient alors $\\theta_n$; on prendra pour estimateur de $\\mathbb{E}[h(X)]$ la quantit√© $M_n(\\theta_n)$ calcul√©e √† partir des m√™mes tirages $X_1, \\cdots, X_n$ que ceux utilis√©s pour le calcul de $G_n$ et $H_n$.\n",
    "\n",
    "Les r√©sultats suivants, qui peuvent √™tre d√©montr√©s sous hypoth√®ses d'int√©grabilit√© sur la fonction $h$, assurent que la m√©thode fonctionne (voir l'article [Robust adaptive importance sampling for normal random vectors](https://projecteuclid.org/euclid.aoap/1255699541)): \n",
    "- (i) $M_n(\\theta_n)$ tend presque s√ªrement vers $\\mathbb{E}[h(X)]$ lorsque $n \\to \\infty$, \n",
    "- (ii) si $\\mathrm{Var}(h(X))>0$, la suite de v.a.\n",
    "$$\n",
    "    \\sqrt{\\frac{n}{\\mathcal{V}(\\theta_n)-M_n(\\theta_n)^2}}(M_n(\\theta_n)-\\mathbb{E}[h(X)])\n",
    "$$\n",
    "converge en loi vers une loi gaussienne standard,\n",
    "- (iii) $\\sqrt{n}(\\theta_n-\\theta_\\star)$ est asymptotiquement gaussienne centr√©e (sa variance peut √™tre exprim√©e, de fa√ßon relativement complexe, en fonction des quantit√©s caract√©risant le probl√®me). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "#### <span style=\"color:blue\">Question 8. </span>  \n",
    "On se limitera dans cette question au cas $d=1$.  On s'int√©resse au cas particulier $h : x \\mapsto \\left(e^{x} - K\\right)^+$, qui correspond au calcul du prix d'une option call en finance (pour $y\\in{\\mathbb{R}}$, on note $y^+:=\\max(y,0)$). \n",
    "Il s'agit d'un exemple jouet puisque $\\mathbb{E}[h(X)]$ a une expression explicite donn√©e par \n",
    "<a id='eq:BS'></a>\n",
    "$$ \\tag{10}\n",
    "    \\mathbb{E}[h(X)] = e^{\\frac{1}{2}} \\, \\Phi(1-\\ln(K))-K \\, \\Phi(-\\ln(K)), \\qquad \\text{avec} \\quad \\Phi(x):=\\P(X\\le x).\n",
    "$$\n",
    "(cette formule n'est plus valable en dimension $d$ sup√©rieure √† $1$).\n",
    "\n",
    "Fixer $n_\\mathrm{max} = 10^6$ et simuler $X_1, \\cdots, X_{n_\\mathrm{max}}$ v.a.  i.i.d. $\\mathcal{N}(0,1)$.\n",
    "\n",
    "**<span style=\"color:blue\">8.a)</span>** Pour diff√©rentes valeurs de $n \\leq n_\\mathrm{max}$, repr√©senter la fonction\n",
    "$$\n",
    "    \\mathcal{V}_n: \\theta \\mapsto \\frac{1}{n} \\sum_{k=1}^n h^2(X_k) \\exp\\left( - \\theta^\\top X_k + \\|\\theta\\|^2/2 \\right).\n",
    "$$\n",
    "  \n",
    "  Tracer aussi la suite $\\{t_k, 1\\leq k \\leq \\min\\{\\tau_\\varepsilon, 100\\}\\}$, o√π $\\tau_\\varepsilon$ est le premier $k$ tel que $\\|G_n(t_k)\\|\\le \\varepsilon$.  Comment cette suite converge-t-elle ?\n",
    "  \n",
    "On pourra prendre $K=1$, se limiter √† tracer $\\mathcal{V}_n$ sur l'intervalle $[0,3]$ et prendre $\\varepsilon = 0.001$.  <!--\n",
    "Noter que lorsque $d=1$,\n",
    "$$\n",
    "    \\theta-\\frac{G_n(\\theta)}{H_n(\\theta)} = \n",
    "    \\frac{\\sum_{k=1}^n h^2(X_k) \\left(\\theta^3+(1-2\\theta^2)X_k+\\theta X_k^2 \\right) e^{-\\theta X_k}}\n",
    "    {\\sum_{k=1}^n h^2(X_k) \\left( 1+(\\theta-X_k)^2 \\right) e^{-\\theta X_k}}.\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "\n",
    "**<span style=\"color:blue\">8.b)</span>** Tracer sur un m√™me graphique les suites $\\{M_n(0), n \\geq 0\\}$ et $\\{M_n(\\theta_n), n \\geq 0\\}$ pour $n$ variant de $100$ √† $n_{\\mathrm{max}}/10$ par sauts de $1000$ (de fa√ßon √† acc√©l√©rer l'algorithme). \n",
    "\n",
    "Pour comparaison, tracer aussi la droite horizontale $y=\\mathbb{E}[h(X)]$ o√π cette constante est calcul√©e avec la formule [(10)](#eq:BS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "**<span style=\"color:blue\">8.c)</span>** Pour diff√©rentes valeurs de $n$, simuler un grand nombre de r√©alisations de $M_n(0)$ et de $M_n(\\theta_n)$, estimer les variances de ces deux v.a., et tracer l'histogramme de ces r√©alisations, ainsi que la droite verticale $x=\\mathbb{E}[h(X)]$. Que nous disent ces histogrammes sur l'efficacit√© des deux m√©thodes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### <span style=\"color:blue\">Question 9. </span>  \n",
    "\n",
    "Reprendre la question pr√©c√©dente en dimension $d$ sup√©rieure, pour la fonction \n",
    "$$\n",
    "    f: x =(x_1,\\dots,x_d)\\mapsto \\left(\\frac{1}{d}\\sum_{i=1}^d e^{x_i}-K\\right)^+,\n",
    "$$ \n",
    "qui correspond au calcul du prix d'un call sur moyenne.  On affichera le trac√© des suites $\\{M_n(\\theta_n), n \\geq 0\\}$ et $\\{M_n(0), n \\geq 0\\}$ ainsi que les histogrammes.\n",
    "\n",
    "Vous pourrez utiliser les fonctions d'alg√®bre lin√©aire `numpy.dot`, `numpy.inner`, `np.eye`, `np.outer`, aisi que `numpy.linalg.inv` et `numpy.linalg.norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
