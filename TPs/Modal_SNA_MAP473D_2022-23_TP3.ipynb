{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modal SNA MAP473D, Ecole Polytechnique, 2022-23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Soumission du TP pour évaluation:** \n",
    "- Remplir ce notebook et déposer sur le moodle le fichier notebook \".ipynb\" ainsi qu'une sauvegarde (export) au format \".html\".\n",
    "- Les réponses aux questions théoriques peuvent être saisies (en latex) dans le notebook; un (seul) fichier au format .pdf contenant le scan de vos réponses manuscrites est aussi accepté.\n",
    "- Les dépôts sur le moodle doivent être faits avant le vendredi 24 mars, 18h. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Noms du binôme:** _A REMPLIR_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3 - Echantillonnage d’importance : changements de probabilités gaussiens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\R{\\mathbb{R}}$\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "$\\def\\mc{\\mathcal}$\n",
    "$\\def\\E{\\mathbb{E}}$\n",
    "\n",
    "## Partie 1. Rappel de cours : changements de probabilités, échantillonnage préférentiel et grandes déviations\n",
    "Soit $X$ une v.a. à valeur dans $\\mathbb{R}^d$, définie sur un espace de probabilité $(\\Omega, \\mathcal{F}, \\P)$ et de loi $\\nu$ sous $\\P$.  On notera $\\mathbb{E}$ l'espérance associée à la probabilité $\\P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\R{\\mathbb{R}}$\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "$\\def\\Q{\\mathbb{Q}}$\n",
    "$\\def\\mc{\\mathcal}$\n",
    "$\\def\\E{\\mathbb{E}}$\n",
    "$\\def\\rmd{\\mathrm{d}}$\n",
    "\n",
    "\n",
    "### 1.1 Principe général du changement de probabilité\n",
    "\n",
    "Supposons que l'objectif soit de calculer \n",
    "$$\n",
    "\\mathbb{E}\\left[ h(X) \\right] = \\int h(x) \\, \\nu(\\mathsf{d} x)\n",
    "$$\n",
    "pour une fonction $h$ mesurable telle que cette espérance est bien définie. Nous supposons être dans une situation où cette intégrale n'est pas explicite. \n",
    "\n",
    "Une approximation numérique basée sur la méthode de Monte  Carlo standard consiste à définir l'estimateur\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{k=1}^N h(X_k) \\qquad \\qquad X_k \\stackrel{i.i.d.}{\\sim} \\nu(\\mathsf{d} x).\n",
    "$$\n",
    "La variance de cet estimateur est \n",
    "$$\n",
    "\\frac{1}{N} \\mathrm{Var}(h(X)) = \\frac{1}{N} \\left( \\int h^2(x) \\nu(\\mathsf{d} x) - \\left(  \\int h(x) \\nu(\\mathsf{d} x) \\right)^2 \\right).\n",
    "$$\n",
    "Dans la suite, nous serons dans des situations où cet estimateur Monte Carlo standard a un pauvre comportement, et envisagerons un changement de loi pour définir une méthode Monte Carlo plus efficace : \n",
    "\n",
    "- nous chercherons une loi $\\tilde \\nu$, telle que $\\nu$ possède une densité $L$ par rapport à la loi $\\tilde \\nu$; \n",
    "$$\n",
    "\\nu(\\mathsf{d} x) = L(x) \\, \\tilde \\nu(\\mathsf{d} x) \\qquad \\text{avec} \\ \\ \\int L(x) \\tilde \\nu(\\mathsf{d} x) = 1.\n",
    "$$\n",
    "- nous exploiterons la relation \n",
    "$$\\tag{1}\n",
    "\\mathbb{E}\\left[h(X) \\right] = \\int h(x) \\nu(\\mathsf{d} x) = \\int h(x) \\  L(x) \\ \\tilde \\nu(\\mathsf{d} x) = \\mathbb{E}_{\\tilde \\nu} \\left[h(X)  \\  L(X)\\right]\n",
    "$$ \n",
    "où sous $\\P_{\\tilde \\nu}$, $X \\sim \\tilde \\nu$. \n",
    "\n",
    "- nous en déduirons un nouvel estimateur Monte Carlo de la quantité inconnue $\\mathbb{E}[h(X)]$ : \n",
    "$$\\tag{2}\n",
    "\\frac{1}{N} \\sum_{k=1}^N h(X_k) \\, L(X_k) \\qquad X_k \\stackrel{i.i.d.}{\\sim} \\tilde \\nu. \n",
    "$$\n",
    "La variance de cet estimateur est \n",
    "$$\n",
    "\\frac{1}{N}\\mathrm{Var}_{\\tilde \\nu}(h(X) \\, L(X)) = \\frac{1}{N} \\left( \\int h^2(x) L^2(x) \\,  \\tilde \\nu(\\mathsf{d} x) - \\left(  \\int h(x) \\nu(\\mathsf{d} x) \\right)^2 \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\P{\\mathbb{P}}$\n",
    "$\\def\\Q{\\mathbb{Q}}$\n",
    "$\\def\\Vth{\\mathrm{Var}_\\theta}$\n",
    "\n",
    "### 1.2 Changements de probabilité inspirés par les grandes déviations (cas $d=1$)<a id='sec:12'></a>\n",
    "Dans ce qui suit, nous allons considérer un changement de loi de la forme\n",
    "$$\\tag{3}\n",
    "\\nu_\\theta(\\mathsf{d} x) \\propto \\exp(\\theta x ) \\nu(\\mathsf{d} x) = \\frac{\\exp(\\theta x )}{\\int \\exp(\\theta y ) \\nu(\\mathsf{d} y)} \\,  \\nu(\\mathsf{d} x)=  \\frac{\\exp(\\theta x )}{\\mathbb{E}[\\exp(\\theta X)]}\\,  \\nu(\\mathsf{d} x)\n",
    "$$\n",
    "pour $\\theta \\in \\mathcal{D} \\subseteq \\mathbb{R}$, où $\\mathcal{D}$ est défini comme l'ensemble de valeurs pour lesquelles la constante de normalisation est définie (nous considérons des lois $\\nu$ pour lesquelles cet ensemble est non vide)\n",
    "$$ \n",
    "    \\mathcal{D} := \\{ \\theta \\in \\mathbb{R}~:  \\mathbb{E}\\!\\left[e^{\\theta X} \\right]<\\infty \\}.\n",
    "$$  \n",
    "On peut montrer que $\\mathcal{D}$ est convexe. \n",
    "\n",
    "Pour tout $\\theta \\in \\mathcal{D}$, la densité de $\\nu$ par rapport à $\\nu_\\theta$ est\n",
    "$$\n",
    "L_\\theta(x) := \\exp(- \\theta x) \\exp\\left( \\ln \\mathbb{E}\\left[ \\exp(\\theta X) \\right] \\right) = \\exp(-\\theta x + \\psi(\\theta))\n",
    "$$\n",
    "en ayant défini la _fonction génératrice des cumulants_ de $\\nu$ par\n",
    "$$\n",
    " \\psi(\\theta) := \\log \\mathbb{E}\\!\\left[ e^{\\theta X} \\right] = \\log \\int \\exp(\\theta x) \\, \\nu(\\mathsf{d} x).\n",
    "$$\n",
    "Nous nous poserons la question du choix adequat de la valeur $\\theta$ parmi cet ensemble $\\mathcal{D}$.\n",
    "\n",
    "\n",
    "\n",
    "Nous introduisons une notation pour traduire le fait que nous considérons des v.a. sous la loi $\\nu_\\theta$ : nous écrirons pour tout fonction mesurable bornée $h$\n",
    "$$\n",
    "\\mathbb{E}_\\theta\\left[h(X)\\right] : = \\int h(x) \\, \\nu_\\theta(\\mathsf{d} x).\n",
    "$$\n",
    "Avec ces notations, la relation  (1)  s'écrit\n",
    "$$\n",
    "\\mathbb{E}\\left[h(X)\\right] = \\mathbb{E}_\\theta \\left[ h(X) \\, L_\\theta(X) \\right] = \\int h(x) \\exp(-\\theta x + \\psi(\\theta))  \\ \\nu_\\theta(\\mathsf{d} x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On peut montrer que pour tout $\\theta$ dans l'intérieur de $\\mathcal{D}$, l'espérance et la variance de le nouvelle loi $\\nu_\\theta$ sont liées aux dérivées successives de $\\psi$ (voir par exemple le Corollaire 7.2  dans __[Information and Exponential Families](https://onlinelibrary.wiley.com/doi/book/10.1002/9781118857281)__) \n",
    "<a id='eq_esp_var2'></a>\n",
    "$$\\tag{4}\n",
    "    \\mathbb{E}_\\theta[X]=\\psi'(\\theta),\\qquad \\qquad \n",
    "    \\mathrm{Var}_\\theta(X)=\\psi''(\\theta);\n",
    "$$\n",
    "et en particulier, pour $\\theta = 0$,\n",
    "$$ \n",
    "    \\mathbb{E}[X]=\\psi'(0),\\qquad \\qquad \n",
    "    \\mathrm{Var}(X)=\\psi''(0).\n",
    "$$\n",
    "\n",
    "Le succès de cette technique de changement de loi dépend de la capacité à identifier la loi $\\nu_\\theta$ et à faire des tirages i.i.d. sous cette loi.  Par exemple, nous montrerons en section 3.1 que si $\\nu$ est la loi $\\mathcal{N}(0,1)$, alors $\\nu_\\theta$ est la loi  $\\mathcal{N}(\\theta,1)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "### 1.3 Application à l'approximation Monte Carlo de la fonction de survie\n",
    "Supposons que l'objectif soit d'approcher la quantité $$\\P[X>a] = \\mathbb{E}\\left[1_{X > a} \\right]$$ par une méthode de Monte Carlo. \n",
    "\n",
    "\n",
    "L'approximation $(2)$ devient\n",
    "<a id='eq:MC:Ptheta'></a>\n",
    "$$ \\tag{5}\n",
    "    \\P(X>a) \\simeq \\frac{1}{n} \\sum_{k=1}^n \\exp\\left( \\psi(\\theta) - \\theta \\, \\tilde X_k \\right) \\, 1_{\\tilde X_k>a}, \\qquad \\qquad \\tilde X_k \\stackrel{i.i.d.}{\\sim} \\nu_\\theta.\n",
    "    $$ \n",
    " \n",
    "\n",
    "La difficulté pour l'utilisateur est de choisir la valeur $\\theta \\in \\mathcal{D}$ la plus \"efficace\".  La relation [(4)](#eq_esp_var2) suggère de choisir $\\theta$ égal à la solution de $\\psi'(\\theta) = a$ (valeur que nous noterons $\\theta_a$), puisqu'avec ce choix, la nouvelle loi $\\nu_\\theta$ est centrée en $a$. Ce n'est pas nécessairement le choix optimal - selon un critère d'optimalité basé sur la réduction de variance (voir section 3.1) - mais cela donne un échantillonneur Monte Carlo plus efficace que l'échantillonneur Monte Carlo standard puisqu'il a plus de chances de produire des points $\\tilde X_k$ qui vérifient la condition $\"x > a\"$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "## Partie 2. Echantillonnage d'importance pour des v.a. de Bernoulli\n",
    "Les v.a. sont définies sur $(\\Omega, \\mathcal{A}, \\P)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\P{\\mathbb{P}}$\n",
    "#### <span style=\"color:blue\">Question 1 (théorique). </span> \n",
    "Sous $\\mathbb{P}_{p_\\cdot}$, $X_1, \\cdots, X_n$ sont des v.a. indépendantes de loi de Bernoulli $B(p_1), \\ldots, B(p_n)$.\n",
    "\n",
    "Sous $\\mathbb{P}_{q_\\cdot}$, $X_1, \\ldots, X_n$ des v.a. indépendantes de lois de Bernoulli $B(q_1), \\ldots, B(q_n)$.\n",
    "\n",
    "Montrer que pour toute fonction $h:\\{0,1\\}^n \\to\\mathbb{R}$ mesurable bornée, on a\n",
    "\\begin{eqnarray*} \n",
    "  \\mathbb{E}_{p_\\cdot} \\left[ h(X_1, \\ldots, X_n) \\right] &=& \\mathbb{E}_{q_\\cdot} \\left[ h(X_1, \\ldots, X_n) \\, \\prod_{i=1}^n \\left(1_{X_i=0}\\frac{1-p_i}{1-q_i}+ 1_{X_i=1}\\frac{p_i}{q_i}\\right) \\right]\n",
    "  \\\\\n",
    "                                        &=& \\left( \\prod_{i=1}^n\\frac{1-p_i}{1-q_i} \\right) \\  \\mathbb{E}_{q_\\cdot}\\left[ h(X_1, \\ldots,X_n) \\ \\prod_{i=1}^n\\left(\\frac{p_i(1-q_i)}{q_i(1-p_i)}\\right)^{X_i} \\right]\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Réponse:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "***\n",
    "#### <span style=\"color:blue\">Question 2 (théorique). \n",
    "    \n",
    "Soit $n\\ge 1$, $p,q\\in ]0,1[$; sous $\\mathbb{P}_p$,  $S \\sim \\operatorname{Binom}(n,p)$ et sous $\\mathbb{P}_q$, $S \\sim \\operatorname{Binom}(n,q)$.\n",
    "\n",
    "\n",
    "Déduire de la question précédente que pour toute fonction $h:\\{0, \\ldots, n\\} \\to \\mathbb{R}$, on a\n",
    "$$\n",
    "    \\mathbb{E}_p \\left[ h(S) \\right] \n",
    "    = \\mathbb{E}_q \\left[ h(S) \\ \\left(\\frac{p}{q}\\right)^{S}\\left(\\frac{1-p}{1-q}\\right)^{n-S} \\right] \n",
    "    = \\left(\\frac{1-p}{1-q}\\right)^n \\mathbb{E}_q\\left[h(S) \\ \\left(\\frac{p(1-q)}{q(1-p)}\\right)^{S} \\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Réponse:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\P{\\mathbb{P}}$\n",
    "***\n",
    "#### <span style=\"color:blue\">Question 3. </span> \n",
    "\n",
    "Soit $S \\sim \\operatorname{Binom}(n,p)$ et $x \\in ]0,p[$.  Estimer $\\P(S \\le nx)$ des trois façons suivantes:\n",
    "- en utilisant la fonction de calcul exact de cette probabilité `scipy.stats.binom.cdf`,\n",
    "- par un Monte Carlo standard en simulant $N$ copies de $S$ de loi $\\operatorname{Binom}(n,p)$ avec la fonction `np.random.binomial` ou `scipy.stats.binom.rvs`,\n",
    "- en utilisant la question 2 et en simulant $N$ copies de $S$ de loi $\\operatorname{Binom}(n,q)$.\n",
    "\n",
    "Dans les deux derniers cas, on donnera un intervalle de confiance asymptotique de probabilité de couverture $0.95$, ainsi que l'erreur relative.\n",
    "\n",
    "On pourra choisir par exemple $n=300$, $p=0.25$, $x=0.1$, $q=x$ et $N=10^4$; et recommencer avec $N=10^6$. \n",
    "\n",
    "Plus généralement, proposer une analyse qui justifie la choix de $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "## Partie 3. Changement de probabilité dans un modèle gaussien\n",
    "Dans cette section, $\\nu$ est  la loi $\\mathcal{N}(0,1)$.  Sous $\\P$, $X \\sim \\mathcal{N}(0,1)$.\n",
    "\n",
    "\n",
    "Nous cherchons à calculer la probabilité d'évenements de la forme $\\{X>a\\}$ ou $\\{|X|>a\\}$ pour des valeurs assez élevées de $a$ de sorte que l'événement puisse être qualifié de rare. \n",
    "\n",
    "Nous avons la relation, pour tout $a>0$,\n",
    "$$\n",
    "    \\frac{1}{a+1/a} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{a^2}{2}} \n",
    "    \\le \\P(X>a) \\le \n",
    "    \\frac{1}{a} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{a^2}{2}},\n",
    "$$\n",
    "ce qui entraine quand $a \\to +\\infty$,\n",
    "$$ \n",
    "    \\P(X>a) \\simeq \\frac{1}{a }\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{a^2}{2}}.\n",
    "$$\n",
    "Ainsi, comme $e^{\\frac{2.15^2}{2}} \\simeq 10$, $\\P(X>a)$ et $\\P(|X|>a)$ sont de l'ordre de $10^{-\\left(\\frac{a}{2.15}\\right)^2}$: des valeurs de $a$ comprises dans l'intervalle $[5,8]$ correspondent à notre champ d'investigation. \n",
    "\n",
    "\n",
    "Nous considérons deux changements de probabilité pour améliorer le comportement de l'échantillonneur de Monte Carlo naïf donné par \n",
    "$$\\tag{6}\n",
    "    \\P(X>a) \\simeq \\frac{1}{n} \\sum_{k=1}^n 1_{X_k > a} \\qquad  X_k \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(0,1).\n",
    "$$\n",
    "$$ \n",
    "\\P(|X|>a) \\simeq \\frac{1}{n} \\sum_{k=1}^n 1_{|X_k| > a} \\qquad  X_k \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(0,1).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "### 3.1 Changement de probabilité par décentrage\n",
    "Ce changement de probabilité repose sur le changement de loi $\\nu \\to \\nu_\\theta$ décrit en [section 1.2](#sec:12).  \n",
    "\n",
    "La fonction génératrice des cumulants de $\\nu$ est définie sur $\\mathcal{D} = \\mathbb{R}$ et vaut $\\psi(\\theta) =\\theta^2/2$. \n",
    "\n",
    "Identifions la loi $\\nu_\\theta$: la relation [(3)](#eq:Principe:IS) montre que pour toute fonction $h:\\mathbb{R} \\to \\mathbb{R}_+$ mesurable\n",
    "\\begin{align*}\n",
    " \\int h(x) \\ \\nu_\\theta(\\mathsf{d} x)   &= \\frac{1}{\\sqrt{2\\pi}} \\int_{\\mathbb{R}} h(x) \\exp \\left(\\theta x - \\theta^2/2 \\right) \\exp(-x^2/2) \\mathsf{d} x  \\\\\n",
    "  &= \\frac{1}{\\sqrt{2\\pi}} \\int_{\\mathbb{R}} \\, h(x) \\exp \\left(-(x-\\theta)^2/2 \\right) \\, \\mathsf{d} x.\n",
    "\\end{align*}\n",
    "On en déduit que $\\nu_\\theta$ est la loi $\\mathcal{N}(\\theta,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "#### <span style=\"color:blue\">Question 4. </span> \n",
    "Simuler $n \\gg 1$ gaussiennes indépendantes de loi $\\mathcal{N}(0,1)$ avec lesquelles on estimera $\\P(X>a)$ d'abord par l'estimateur naïf [(6)](#eq:MC:naif), puis par l'estimateur [(5)](#eq:MC:Ptheta) appliqué avec $\\theta = \\theta_a$ où \n",
    "$$ \\theta_a: \\text{solution de} \\qquad \\psi'(\\theta) = a. $$  \n",
    "\n",
    "- Comparer à la valeur exacte de $\\P(X>a)$.  \n",
    "- Pour chacun des estimateurs, utiliser le TCL pour construire un intervalle de confiance asymptotique de probabilité de couverture $95 \\%$. \n",
    "- Lorsque cela s'y prête, regarder la qualité de l'estimation par exemple en calculant l'erreur relative (taille de l'intervalle de confiance divisée par l'estimation ponctuelle).\n",
    "- Expliquez comment vous avez choisi le nombre de termes $n$ pour construire ces estimateurs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "#### <span style=\"color:blue\">Question 5 (théorique). </span> \n",
    "\n",
    "La longueur de l'intervalle de confiance est une fonction croissante de la variance de l'estimateur : on veut donc choisir $\\theta$ qui minimise cette variance.  Nous allons montrer que le choix de $\\theta$ fait dans la question précédente pourrait être amélioré si le critère d'optimalité souhaité est de minimiser la variance du nouvel estimateur.\n",
    "\n",
    "a) Montrer que choisir $\\theta$ qui minimise la variance de l'estimateur donné par [(5)](#eq:MC:Ptheta), revient à choisir $\\theta$ qui minimise\n",
    "$$\n",
    "    \\theta \\mapsto \\mathbb{E}\\left[ 1_{X > a} \\exp\\left(\\frac{\\theta^2}{2} - \\theta X \\right) \\right].\n",
    "$$\n",
    "\n",
    "b) Montrer que cette fonction est convexe et qu'elle possède un unique minimum atteint en $\\theta_\\star$ solution de\n",
    "$$\n",
    "\\text{$\\theta$ tel que }  \\qquad  \\mathbb{E}\\left[ 1_{X > a} (\\theta - X) \\exp\\left(- \\theta X \\right) \\right] = 0;\n",
    "$$\n",
    "on admettra que l'on peut permuter dérivée et intégration.\n",
    "\n",
    "c) En déduire que prendre $\\theta = \\theta_a$ est meilleur que prendre $\\theta = 0$; et que l'on a $\\theta_\\star > \\theta_a$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Réponse:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "\n",
    "### 3.2 Changement de probabilité par modification de la variance\n",
    "Le changement de probabilité par modification de la variance repose sur l'observation suivante : pour toute fonction $h: \\mathbb{R} \\to \\mathbb{R}$ mesurable bornée, on a\n",
    "$$\n",
    "    \\mathbb{E}[h(X)] =  \\frac{1}{\\sqrt{2 \\pi}} \\int  h(y) e^{- \\frac{1}{2} y^2} \\,  \\mathsf{d} y =   \\frac{\\sigma}{ \\sigma \\sqrt{2 \\pi}} \\int  h(y) e^{- \\frac{1}{2} y^2  + \\frac{1}{2\\sigma^2} y^2} \\,e^{-  \\frac{1}{2\\sigma^2} y^2}  \\mathsf{d} y = \\sigma \\, \\mathbb{E}_\\sigma \\left[h(X)     e^{- \\frac{1}{2} X^2  + \\frac{1}{2\\sigma^2} X^2} \\right]\n",
    "$$\n",
    "où sous $\\P_\\sigma$,  $X \\sim \\mathcal{N}(0, \\sigma^2)$. \n",
    "\n",
    "On peut donc estimer $\\mathbb{E}[h(X)]$ par \n",
    "<a id='eq:MC:Psi'></a>\n",
    "$$ \n",
    "    \\mathbb{E}[h(X)] \\simeq \\frac{\\sigma}{n} \\sum_{k=1}^n \\, h(Y_k) \\, e^{-\\frac{(\\sigma^2-1)}{2 \\sigma^2} Y_k^2} , \\qquad\n",
    "   Y_k \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(0,\\sigma^2).\n",
    "$$\n",
    "Là encore, le choix de $\\sigma$ n'est pas anodin sur la qualité de l'estimateur. Chercher $\\sigma$ qui minimise la variance de l'estimateur i.e.\n",
    "$$\n",
    "\\frac{1}{n} \\mathrm{Var}_\\sigma \\left( \\sigma \\, h(Y) \\, e^{-\\frac{(\\sigma^2-1)}{2 \\sigma^2} Y^2}\\right)\n",
    "$$\n",
    "est équivalent à minimiser le moment d'ordre $2$  (puisque l'estimateur est sans biais) soit \n",
    "$$\n",
    "\\sigma^2 \\, \\mathbb{E}_\\sigma \\left[ h^2(Y) \\, e^{-\\frac{(\\sigma^2-1)}{\\sigma^2} Y^2}\\right] = \\sigma^2 \\, \\mathbb{E}\\left[ h^2(\\sigma X) \\, e^{-(\\sigma^2-1) X^2}\\right]. \n",
    "$$\n",
    "On choisira $\\sigma = \\sigma_\\star$ où $\\sigma_\\star$ est solution de\n",
    "$$\n",
    "    \\operatorname{argmin}_{\\sigma > 0} \\sigma^2 \\ \\mathbb{E} \\left[ h^2(\\sigma X)  \\ e^{-(\\sigma^2-1)X^2} \\right].\n",
    "$$\n",
    "Bien souvent, on n'a pas d'expression explicite de cette valeur optimale, et la détermination de $\\sigma_\\star$ est une des principales difficultés de l'implémentation de cette méthode. Dans l'application numérique suivante, nous utiliserons une technique rudimentaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "#### <span style=\"color:blue\">Question 6. </span> \n",
    "Se donner $\\mathcal{S}$, une version discrétisée de l'intervalle $\\left[\\frac{a}{2}, 3a \\right]$.  \n",
    "\n",
    "Simuler $n_1$ gaussiennes centrées réduites.\n",
    "\n",
    "Avec ce même jeu de simulations, pour chaque valeur $\\sigma \\in \\mathcal{S}$, calculer une approximation Monte Carlo de la quantité \n",
    "$$\n",
    "    \\sigma^2 \\ \\mathbb{E} \\left[ 1_{|\\sigma X| >a} e^{-(\\sigma^2-1)X^2} \\right].\n",
    "$$\n",
    "En déduire une approximation $\\hat{\\sigma}_\\star$ de $\\sigma_\\star$.\n",
    "\n",
    "*Indication : on pourra considérer une version discrétisée de l'intervalle en $300$ points équidistants. Puis on pourra relancer la recherche sur un intervalle plus réduit. Commencez par prendre $n=1e4$ puis l'augmenter - jusqu'à prendre $𝑛_1$ de l'ordre de $1𝑒6$ et même plus grand. Commentez*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\P{\\mathbb{P}}$\n",
    "#### <span style=\"color:blue\">Question 7. </span>  \n",
    "Simuler $n_2$ autres gaussiennes centrées réduites $X_1, \\dots, X_{n_2}$ ($n_2 \\gg n_1$). \n",
    "\n",
    "Avec ce nouveau jeu de simulations, \n",
    "- proposer une estimation de $\\P(|X| >a)$ via la méthode de Monte Carlo naïve; \n",
    "- puis une estimation via la formule\n",
    "$$\n",
    "    \\P(|X|>a) \\simeq \\frac{\\sigma}{n_2} \\sum_{k=1}^{n_2} 1_{|\\sigma X_k| > a} e^{-\\frac{(\\sigma^2-1)}{2} X_k^2} \\qquad X_k \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(0,1)\n",
    "$$\n",
    "tout d'abord pour $\\sigma = \\hat{\\sigma}_\\star$ puis pour $\\sigma = a$. Cette formule utilise le fait que si $X \\sim \\mathcal{N}(0,1)$ alors $\\sigma X \\sim \\mathcal{N}(0, \\sigma^2)$. \n",
    "\n",
    "Pour chacun des cas,  \n",
    "- utiliser le TCL pour construire un intervalle de confiance asymptotique de probabilité de couverture $95\\%$.  \n",
    "- afficher l'erreur relative, lorsqu'elle est bien définie.\n",
    "\n",
    "Comparer ces stratégies, par exemple en regardant l'erreur relative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "## Partie 4. Algorithme adaptatif de moyenne optimale dans le cas gaussien multidimensionnel\n",
    "\n",
    "Dans cette section, $I_d$ désigne la matrice identité de taille $d \\times d$. On note  $\\|\\cdot \\|$ la norme euclidienne associée. Pour un vecteur $a$, $a^\\top$ est la transposée. Par convention, les vecteurs sont des vecteurs colonnes.\n",
    "\n",
    "Sous $\\P$, $X$ est un vecteur gaussien standard de $\\mathbb{R}^d$ : $\\nu$ est donc la loi $\\mathcal{N}(0,I_d)$. \n",
    "\n",
    "On cherche à calculer $\\mathbb{E}[h(X)]$ pour une fonction $h : \\mathbb{R}^d \\to \\mathbb{R}$ mesurable telle que pour tout $\\theta\\in \\mathbb{R}^d$,\n",
    "$$\n",
    "    0<\\mathbb{E} \\left[h^2(X) \\ e^{- \\theta^\\top X} \\right]<\\infty.\n",
    "$$\n",
    "Pour ce faire, on peut implémenter un estimateur de Monte Carlo naïf ou exploiter la relation\n",
    "<a id='eq:IS:multidim'></a>\n",
    "$$ \\tag{7}\n",
    "  \\mathbb{E}[h(X)]=\\mathbb{E} \\left[h(X + \\theta) \\, e^{- \\theta^\\top  X-\\|\\theta\\|^2/2} \\right],\n",
    "$$ \n",
    "valable pour tout $\\theta \\in \\mathbb{R}^d$, et toute fonction $h: \\mathbb{R}^d \\to \\mathbb{R}_+$ mesurable. L'objectif de cette section est d'apprendre, par une procédure adaptative, la valeur optimale du paramètre $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "\n",
    "#### Algorithme de Lelong et Jourdain\n",
    "D'après l'article [Robust adaptive importance sampling for normal random vectors](https://projecteuclid.org/euclid.aoap/1255699541). On déduit de [(7)](#eq:IS:multidim) que pour tout $\\theta \\in \\mathbb{R}^d$,\n",
    "- (i) la v.a.\n",
    "<a id='eq:IS:JL'></a>\n",
    "$$ \\tag{8}\n",
    "    M_n(\\theta) := \\frac{1}{n} \\sum_{k=1}^n h(X_k + \\theta) \\ e^{-\\theta^\\top X_k -\\|\\theta\\|^2/2}, \n",
    "    \\qquad  X_k \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(0,1)\n",
    "$$\n",
    "est un estimateur sans biais de l'objectif $\\mathbb{E}\\left[h(X)\\right]$,\n",
    "- (ii) sa variance est égale à $n^{-1} \\left( \\mathcal{V}(\\theta)- \\left(\\mathbb{E}[h(X)] \\right)^2 \\right)$, où\n",
    "$$\n",
    "    \\mathcal{V}(\\theta) := \\mathbb{E} \\left[h^2(X) \\,  e^{- \\theta^\\top X +\\|\\theta\\|^2/2} \\right].\n",
    "$$\n",
    "\n",
    "On souhaite donc appliquer l'estimateur [(8)](#eq:IS:JL) avec $\\theta \\leftarrow \\theta_\\star$ où $\\theta_\\star$ minimise $\\mathcal{V}(\\theta)$. Cependant, en général, lorsque $\\mathbb{E}[h(X)]$ est inconnu, il en est de même pour $\\mathcal{V}(\\theta)$ et donc $\\theta_\\star$ est défini comme un minimiseur d'une fonction incalculable. Or, sous des conditions d'intégrabilité sur $h$, la fonction $\\mathcal{V}$ est de classe $C^2$, strictement convexe et possédant un unique minimum $\\theta_\\star$; son gradient $G$ et son Hessien $H$ sont donnés par\n",
    "\\begin{align*}\n",
    "  G(\\theta) & :=  \\mathbb{E} \\left[ h^2(X) \\left( \\theta -X \\right) e^{- \\theta^\\top X +\\|\\theta\\|^2/2} \\right]  \\in \\mathbb{R}^{d \\times 1}, \\\\\n",
    "  H(\\theta) & := \\mathbb{E} \\left[ h^2(X) \\left( I_d + (\\theta-X) (\\theta-X)^\\top \\right) e^{- \\theta^\\top X+\\|\\theta\\|^2/2} \\right]  \\in \\mathbb{R}^{d \\times d}.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Afin d'approcher $\\theta_\\star$, on va utiliser une version bruitée de la *méthode de Newton* : pour $n$ fixé, $\\theta_\\star$ sera approché par $\\theta_n$ défini comme la limite lorsque $k$ tend vers l'infini, de la suite $\\{t_k, k\\ge 0\\}$ satisfaisant à\n",
    "<a id='eq:Newton'></a>\n",
    "$$ \\tag{9}\n",
    "    t_{k+1} = t_k - \\left(H_n(t_k)\\right)^{-1} \\, G_n(t_k)\n",
    "$$\n",
    "où $H_n(t), G_n(t)$ sont les approximations Monte Carlo de $H(t)$ et $G(t)$ calculées à partir des mêmes $n$ réalisations indépendantes $X_1, \\cdots, X_n$ de v.a. de loi $\\mathcal{N}_d(0,I_d)$:\n",
    "\\begin{align*}\n",
    "    G_n(\\theta) & :=  \\frac{1}{n}\\sum_{k=1}^n h^2(X_k) (\\theta-X_k)e^{- \\theta^\\top X_k +\\|\\theta\\|^2/2}, \\\\\n",
    "    H_n(\\theta) & :=  \\frac{1}{n}\\sum_{k=1}^n h^2(X_k) \\left(I_d + (\\theta-X_k)(\\theta-X_k)^\\top\\right) e^{-\\theta^\\top X_k+\\|\\theta\\|^2/2}.\n",
    "\\end{align*}\n",
    "En pratique, on se fixe un seuil $\\varepsilon > 0$ et on itère la relation [(9)](#eq:Newton) tant que $\\|G_n(t_k)\\|>\\varepsilon$. On obtient alors $\\theta_n$; on prendra pour estimateur de $\\mathbb{E}[h(X)]$ la quantité $M_n(\\theta_n)$ calculée à partir des mêmes tirages $X_1, \\cdots, X_n$ que ceux utilisés pour le calcul de $G_n$ et $H_n$.\n",
    "\n",
    "Les résultats suivants, qui peuvent être démontrés sous hypothèses d'intégrabilité sur la fonction $h$, assurent que la méthode fonctionne (voir l'article [Robust adaptive importance sampling for normal random vectors](https://projecteuclid.org/euclid.aoap/1255699541)): \n",
    "- (i) $M_n(\\theta_n)$ tend presque sûrement vers $\\mathbb{E}[h(X)]$ lorsque $n \\to \\infty$, \n",
    "- (ii) si $\\mathrm{Var}(h(X))>0$, la suite de v.a.\n",
    "$$\n",
    "    \\sqrt{\\frac{n}{\\mathcal{V}(\\theta_n)-M_n(\\theta_n)^2}}(M_n(\\theta_n)-\\mathbb{E}[h(X)])\n",
    "$$\n",
    "converge en loi vers une loi gaussienne standard,\n",
    "- (iii) $\\sqrt{n}(\\theta_n-\\theta_\\star)$ est asymptotiquement gaussienne centrée (sa variance peut être exprimée, de façon relativement complexe, en fonction des quantités caractérisant le problème). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "#### <span style=\"color:blue\">Question 8. </span>  \n",
    "On se limitera dans cette question au cas $d=1$.  On s'intéresse au cas particulier $h : x \\mapsto \\left(e^{x} - K\\right)^+$, qui correspond au calcul du prix d'une option call en finance (pour $y\\in{\\mathbb{R}}$, on note $y^+:=\\max(y,0)$). \n",
    "Il s'agit d'un exemple jouet puisque $\\mathbb{E}[h(X)]$ a une expression explicite donnée par \n",
    "<a id='eq:BS'></a>\n",
    "$$ \\tag{10}\n",
    "    \\mathbb{E}[h(X)] = e^{\\frac{1}{2}} \\, \\Phi(1-\\ln(K))-K \\, \\Phi(-\\ln(K)), \\qquad \\text{avec} \\quad \\Phi(x):=\\P(X\\le x).\n",
    "$$\n",
    "(cette formule n'est plus valable en dimension $d$ supérieure à $1$).\n",
    "\n",
    "Fixer $n_\\mathrm{max} = 10^6$ et simuler $X_1, \\cdots, X_{n_\\mathrm{max}}$ v.a.  i.i.d. $\\mathcal{N}(0,1)$.\n",
    "\n",
    "**<span style=\"color:blue\">8.a)</span>** Pour différentes valeurs de $n \\leq n_\\mathrm{max}$, représenter la fonction\n",
    "$$\n",
    "    \\mathcal{V}_n: \\theta \\mapsto \\frac{1}{n} \\sum_{k=1}^n h^2(X_k) \\exp\\left( - \\theta^\\top X_k + \\|\\theta\\|^2/2 \\right).\n",
    "$$\n",
    "  \n",
    "  Tracer aussi la suite $\\{t_k, 1\\leq k \\leq \\min\\{\\tau_\\varepsilon, 100\\}\\}$, où $\\tau_\\varepsilon$ est le premier $k$ tel que $\\|G_n(t_k)\\|\\le \\varepsilon$.  Comment cette suite converge-t-elle ?\n",
    "  \n",
    "On pourra prendre $K=1$, se limiter à tracer $\\mathcal{V}_n$ sur l'intervalle $[0,3]$ et prendre $\\varepsilon = 0.001$.  <!--\n",
    "Noter que lorsque $d=1$,\n",
    "$$\n",
    "    \\theta-\\frac{G_n(\\theta)}{H_n(\\theta)} = \n",
    "    \\frac{\\sum_{k=1}^n h^2(X_k) \\left(\\theta^3+(1-2\\theta^2)X_k+\\theta X_k^2 \\right) e^{-\\theta X_k}}\n",
    "    {\\sum_{k=1}^n h^2(X_k) \\left( 1+(\\theta-X_k)^2 \\right) e^{-\\theta X_k}}.\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "\n",
    "**<span style=\"color:blue\">8.b)</span>** Tracer sur un même graphique les suites $\\{M_n(0), n \\geq 0\\}$ et $\\{M_n(\\theta_n), n \\geq 0\\}$ pour $n$ variant de $100$ à $n_{\\mathrm{max}}/10$ par sauts de $1000$ (de façon à accélérer l'algorithme). \n",
    "\n",
    "Pour comparaison, tracer aussi la droite horizontale $y=\\mathbb{E}[h(X)]$ où cette constante est calculée avec la formule [(10)](#eq:BS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\P{\\mathbb{P}}$\n",
    "\n",
    "**<span style=\"color:blue\">8.c)</span>** Pour différentes valeurs de $n$, simuler un grand nombre de réalisations de $M_n(0)$ et de $M_n(\\theta_n)$, estimer les variances de ces deux v.a., et tracer l'histogramme de ces réalisations, ainsi que la droite verticale $x=\\mathbb{E}[h(X)]$. Que nous disent ces histogrammes sur l'efficacité des deux méthodes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### <span style=\"color:blue\">Question 9. </span>  \n",
    "\n",
    "Reprendre la question précédente en dimension $d$ supérieure, pour la fonction \n",
    "$$\n",
    "    f: x =(x_1,\\dots,x_d)\\mapsto \\left(\\frac{1}{d}\\sum_{i=1}^d e^{x_i}-K\\right)^+,\n",
    "$$ \n",
    "qui correspond au calcul du prix d'un call sur moyenne.  On affichera le tracé des suites $\\{M_n(\\theta_n), n \\geq 0\\}$ et $\\{M_n(0), n \\geq 0\\}$ ainsi que les histogrammes.\n",
    "\n",
    "Vous pourrez utiliser les fonctions d'algèbre linéaire `numpy.dot`, `numpy.inner`, `np.eye`, `np.outer`, aisi que `numpy.linalg.inv` et `numpy.linalg.norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
